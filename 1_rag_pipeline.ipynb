{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](./images/Basic_Rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction \n",
    "\n",
    "The first step in any RAG pipeline is to get the data you want to work with. In our case, we'll be extracting the text content from the Anthropic news website.\n",
    "\n",
    "This code block defines a function bs4_extractor that uses BeautifulSoup to parse the HTML content of a webpage and extract the text. The RecursiveUrlLoader then uses this function to load the content from the specified URL. We set max_depth=2 to limit how deep the scraper will go into the website's links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip() #parse wbpage text\n",
    "\n",
    "# ref: https://python.langchain.com/docs/integrations/document_loaders/\n",
    "loader = RecursiveUrlLoader(\"https://www.anthropic.com/news\", extractor=bs4_extractor, max_depth=2)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs) # Number of documents loaded from the Anthropic News site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the metadata of one of the loaded documents. \n",
    "# The metadata provides useful information about the source of the document, such as the URL, title, and description.\n",
    "\n",
    "# docs[1].page_content\n",
    "docs[10].metadata   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a better sense of the content we've loaded, \n",
    "# this code block defines a helper function wrap_text to format the text and then prints the content of each document.\n",
    "\n",
    "def wrap_text(text, width=80):\n",
    "    return '\\n'.join([text[i:i+width] for i in range(0, len(text), width)]) \n",
    "\n",
    "for doc in docs:\n",
    "    print(wrap_text(doc.page_content))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk the data\n",
    "\n",
    "Now that we have our documents, the next step is to split them into smaller chunks. This is important for a few reasons:\n",
    "\n",
    "__Vector search efficiency__: Smaller chunks are easier to search and retrieve.\n",
    "\n",
    "__Context window limitations__: LLMs have a limited context window, so we need to make sure the retrieved information fits within that window.\n",
    "\n",
    "__Relevance__: Smaller chunks are more likely to be focused on a specific topic, which improves the relevance of the retrieved information.\n",
    "\n",
    "We'll use the `RecursiveCharacterTextSplitter` to split our documents. This splitter tries to split text on a series of characters (like newlines, spaces, etc.) in a recursive manner.\n",
    "\n",
    "- `chunk_size=1000`: This sets the maximum size of each chunk to 1000 characters.\n",
    "\n",
    "- `chunk_overlap=200`: This creates an overlap of 200 characters between consecutive chunks. This helps to ensure that we don't lose any important context at the boundaries of our chunks.\n",
    "\n",
    "- `add_start_index=True`: This will add the starting index of the chunk in the original document to the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk the data\n",
    "\n",
    "# ref: https://python.langchain.com/docs/concepts/text_splitters/\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    print(wrap_text(split.page_content))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[2].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "\n",
    "Now that we have our document chunks, we need to create an index that we can search. We'll use a vector store for this, which allows us to perform semantic search on our documents.\n",
    "\n",
    "In this block, we're setting up our vector store using ChromaDB and Google's Generative AI embeddings.\n",
    "\n",
    "- `embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")`: This initializes the embedding model that will be used to convert our document chunks into numerical vectors.\n",
    "\n",
    "- `vector_store = Chroma(...)`: This creates a ChromaDB vector store.\n",
    "\n",
    "- `collection_name`: A name for our collection of documents.\n",
    "\n",
    "- `embedding_function`: The embedding model to use.\n",
    "\n",
    "- `persist_directory`: The directory where the vector store data will be saved locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing\n",
    "\n",
    "#define the embeddings model\n",
    "#ref: https://python.langchain.com/docs/integrations/text_embedding/\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "#disable telemetry\n",
    "client_settings = chromadb.config.Settings(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    anonymized_telemetry=False,  # Disables telemetry\n",
    ")\n",
    "\n",
    "#define the vector store\n",
    "#ref: https://python.langchain.com/docs/concepts/vectorstores/\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"1_basic_rag_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\",  # Where to save data locally\n",
    "    client_settings=client_settings\n",
    ")\n",
    "\n",
    "# vector_store.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we add our document chunks to the vector store. \n",
    "# This process will convert each chunk into a vector and store it in the database.\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=splits)\n",
    "document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's retrieve a document from the vector store by its ID to confirm that it has been indexed correctly.\n",
    "vector_store.get_by_ids([document_ids[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "With our documents indexed, we can now perform retrieval. The goal of this step is to find the most relevant document chunks for a given user question.\n",
    "\n",
    "First, we'll set up our LLM and a prompt template.\n",
    "\n",
    "- `llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")`: We'll use the Gemini 1.5 Flash model for generation.\n",
    "\n",
    "- `template = ...`: This is the prompt template that we'll use to combine the retrieved context with the user's question. The {context} and {question} are placeholders that will be filled in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure the llm\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")  # web is search disabled by default\n",
    "\n",
    "#set the prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "rag_prompt_template = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a sample user question.\n",
    "user_question = \"What is anthropic?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we'll use the vector store's similarity_search method to find the top 5 most similar documents to the user's question.\n",
    "retrieved_docs = vector_store.similarity_search(user_question, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review the retreived docs and see how relevant they are\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect metadata\n",
    "retrieved_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The similarity_search_with_score method returns the documents along with their similarity scores.\n",
    "vector_store.similarity_search_with_score(user_question, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "The final step is to use the retrieved documents to generate an answer to the user's question.\n",
    "\n",
    "We'll combine the content of the retrieved documents and use our prompt template to create a final prompt for the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate answer\n",
    "\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "prompt = rag_prompt_template.invoke({\"question\": user_question, \"context\": docs_content})\n",
    "response = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generated response\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also include citations in our response by extracting the source from the metadata of the retrieved documents.\n",
    "\n",
    "sources = [doc.metadata[\"source\"] for doc in retrieved_docs]\n",
    "\n",
    "print(f\"Sources: {sources}\\n\\n\")\n",
    "print(f'Answer: {response.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Retreiver\n",
    "\n",
    "LangChain provides a `Retriever` interface, which is a more general way to retrieve documents. A vector store can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "\n",
    "Here, we're creating a retriever from our vector store. We can also specify search arguments like k (the number of documents to retrieve) and search_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://python.langchain.com/docs/concepts/retrievers/\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 100}, search_type='similarity')\n",
    "\n",
    "retrieved_docs = retriever.invoke(user_question)\n",
    "retrieved_docs\n",
    "\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's put it all together in a single function.\n",
    "def generate_answer(user_question):\n",
    "    #retrieve the relevant docs\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 100}, search_type='similarity')\n",
    "    retrieved_docs = retriever.invoke(user_question)\n",
    "    \n",
    "    #generate\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    prompt = rag_prompt_template.invoke({\"question\": user_question, \"context\": docs_content})\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return response.content\n",
    "\n",
    "user_question = \"What is Anthropic?\"\n",
    "generate_answer(user_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
