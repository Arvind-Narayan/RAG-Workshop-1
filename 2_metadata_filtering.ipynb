{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](./images/Metadata_Filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing RAG with Metadata\n",
    "\n",
    "In the first part of this tutorial, we built a basic RAG pipeline. Now, we'll take it a step further by incorporating metadata.\n",
    "\n",
    "__What is metadata?__ It's data about your data. For a webpage, this could include the author, publication date, title, or even a summary. By extracting and indexing this metadata alongside the document content, we can perform more targeted and efficient retrievals.\n",
    "\n",
    "\n",
    "Let's dive in and make our RAG pipeline smarter! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction\n",
    "\n",
    "The first step is to get our data, but this time, we'll be more deliberate about what information we extract. Instead of just grabbing all the text, we'll create custom functions to pull out specific pieces of metadata and the main article content.\n",
    "\n",
    "We'll define two functions:\n",
    "\n",
    "1. `custom_metadata_extractor`: This function will parse the HTML to find the page title, description, language, and any associated \"subjects\" or tags. It uses BeautifulSoup to navigate the HTML structure and find the relevant elements.\n",
    "\n",
    "2. `article_extractor`: This function's job is to find the main article content of the page, which is usually contained within an <article> tag.\n",
    "\n",
    "Using these specific extractors ensures we get clean, relevant data for both content and metadata, which will be crucial for filtering later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metadata_extractor(raw_html: str, url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts custom metadata from the raw HTML of a webpage.\n",
    "\n",
    "    This function parses the HTML to find the title, description, language,\n",
    "    and a list of subjects associated with the page.\n",
    "\n",
    "    Args:\n",
    "        raw_html: The raw HTML content of the page.\n",
    "        url: The URL of the page.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the extracted metadata.\n",
    "    \"\"\"\n",
    "        \n",
    "    metadata = {\"source\": url}\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    if title := soup.find(\"title\"):\n",
    "        raw_title = title.get_text()\n",
    "        clean_title = re.sub(r'\\\\\\s*Anthropic\\s*', '', raw_title)\n",
    "        metadata[\"title\"] = clean_title.strip()\n",
    "\n",
    "    if description := soup.find(\"meta\", attrs={\"name\": \"description\"}):\n",
    "        metadata[\"description\"] = description.get(\"content\", None)\n",
    "    if html := soup.find(\"html\"):\n",
    "        metadata[\"language\"] = html.get(\"lang\", None)\n",
    "    # Extract subjects into a list\n",
    "    subjects = []\n",
    "    subject_container = soup.find('div', class_='PostDetail_post-detail-types-subjects__rYglE')\n",
    "    if subject_container:\n",
    "        subject_spans = subject_container.find_all('span', class_='PostDetail_post-subject__Kpz7U')\n",
    "        subjects = [span.get_text() for span in subject_spans]\n",
    "\n",
    "    metadata[\"subjects\"] = subjects if subjects else ['Other']\n",
    "    metadata[\"subjects\"] = metadata['subjects'][0].lower() #just using the first topic for simplicity\n",
    "    \n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "\n",
    "def article_extractor(html: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the article content and post subjects from the given HTML.\n",
    "\n",
    "    Args:\n",
    "        html: The HTML content as a string.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the extracted article text and a list of subjects.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    # Extract text from the <article> tag\n",
    "    article_tag = soup.find('article')\n",
    "    article_text = ''\n",
    "    if article_tag:\n",
    "        # Clean up the text by removing excessive newlines\n",
    "        article_text = re.sub(r\"\\n\\n+\", \"\\n\\n\", article_tag.get_text()).strip()\n",
    "    \n",
    "\n",
    "    return article_text\n",
    "\n",
    "\n",
    "# Now, we'll use LangChain's RecursiveUrlLoader again, \n",
    "# but this time we'll pass our custom functions to the extractor and metadata_extractor parameters.\n",
    "\n",
    "loader = RecursiveUrlLoader(\"https://www.anthropic.com/news\", extractor=article_extractor, metadata_extractor=custom_metadata_extractor, max_depth=2)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[1].page_content\n",
    "docs[10].metadata   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text, width=80):\n",
    "    return '\\n'.join([text[i:i+width] for i in range(0, len(text), width)]) \n",
    "\n",
    "for doc in docs:\n",
    "    print(wrap_text(doc.page_content))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk the Data\n",
    "\n",
    "Just like in the first tutorial, we need to split our documents into smaller chunks for effective processing by the LLM. We'll use the same RecursiveCharacterTextSplitter with a 1000-character chunk size and a 200-character overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk the data\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs) #Figure out how to explicitly set the metadata for the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    print(wrap_text(split.page_content))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[2].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "Next, we'll create our vector store and index the document chunks. We're using ChromaDB again with Google's embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing\n",
    "\n",
    "#define the embeddings model\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "#define the vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"2_metadata_filtering_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\",  # Where to save data locally\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# vector_store.delete_collection()\n",
    "\n",
    "#add the chunks in to db\n",
    "document_ids = vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retreival & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#configure the llm\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")  \n",
    "\n",
    "#set the prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "rag_prompt_template = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(user_question):\n",
    "    #retrieve the relevant docs\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5}, search_type='similarity')\n",
    "    retrieved_docs = retriever.invoke(user_question)\n",
    "    \n",
    "    #generate\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    prompt = rag_prompt_template.invoke({\"question\": user_question, \"context\": docs_content})\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return retrieved_docs, response.content\n",
    "\n",
    "user_question = \"What is Anthropic?\"\n",
    "retrieved_docs, answer = generate_answer(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval with Metadata Filtering\n",
    "\n",
    "This is where the magic happens! Now that our metadata is indexed, we can use it to create more powerful and precise retrievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = set([doc.metadata['subjects'] for doc in docs ])\n",
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Metadata Filtering\n",
    "\n",
    "You can manually create a filter to narrow down your search. ChromaDB supports a variety of filter operators. In this example, we're creating a filter to only search for documents where the subject is \"announcements\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_filter = {\"subjects\": {\"$in\": ['announcements']}}\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5, 'filter': subject_filter }, search_type='similarity')\n",
    "retrieved_docs = retriever.invoke(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in retrieved_docs:\n",
    "    print(doc.metadata['subjects'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually creating filters is great, but what if we could have the LLM create the filter for us based on the user's query? \n",
    "\n",
    "_ps: You can also use Langchain's Self Query retriver_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What did Anthropic announce about Economic Futures Program?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LLM with structured output to figure out the best topic filter for the query \n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the output schema using a Pydantic model\n",
    "class Subject(BaseModel):\n",
    "    \"\"\"A class to hold the subject of a user query.\"\"\"\n",
    "    subject: str = Field(description=\"The category of the user query\")\n",
    "\n",
    "# Your existing code\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0) # Note: I've updated to a more recent model name\n",
    "\n",
    "# Use the Pydantic model with with_structured_output\n",
    "structured_llm = llm.with_structured_output(Subject)\n",
    "\n",
    "# The rest of your code should now work\n",
    "subject_template = \"\"\"Classify the user query into one of the following article categories:\n",
    "{subjects}.\n",
    "\n",
    "user query: {question}\n",
    "\"\"\"\n",
    "\n",
    "subject_prompt_template  = PromptTemplate.from_template(subject_template)\n",
    "\n",
    "# Assuming 'subjects' is a list of strings\n",
    "subjects = list(subjects)\n",
    "\n",
    "prompt = subject_prompt_template.invoke({\"question\": user_question, \"subjects\": \", \".join(subjects)})\n",
    "subject_response = structured_llm.invoke(prompt)\n",
    "\n",
    "print(subject_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filtered_answer(user_question, subject_filter):\n",
    "    #retrieve the relevant docs\n",
    "    subject_filter_chromadb = {\"subjects\": {\"$in\": [subject_filter]}}\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5, 'filter': subject_filter_chromadb }, search_type='similarity')\n",
    "    retrieved_docs = retriever.invoke(user_question)\n",
    "    \n",
    "    #generate\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    prompt = rag_prompt_template.invoke({\"question\": user_question, \"context\": docs_content})\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return retrieved_docs, response.content\n",
    "\n",
    "retrieved_docs, answer = generate_filtered_answer(user_question, subject_response.subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in retrieved_docs:\n",
    "    print(doc.metadata['subjects'])\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
